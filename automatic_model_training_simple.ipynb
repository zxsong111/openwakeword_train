{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl_FIEj-auGq"
      },
      "source": [
        "## Training your own openWakeWord models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c3bc30b2fd244afebb59ec17c2426094",
            "b8eaff451261467cb335d753da33357b",
            "3cc9e9c896154ded80b87ff24971a5e8",
            "52dd96d97087456cba48a87b2c473d8a",
            "418b934cddf540bb989e2ce797fd1df0",
            "1bc538c08b65471a80089150ddb8e2b9",
            "cbfcfeb2a4384a3a846b6162f75e1df7",
            "ded8a90ee44248f68e317fcda0903634",
            "46c1805f82df4228b1e8eddd1d5ef00b",
            "b6f3a8bc9c084e5aa3f5157311c2b188",
            "7a870a47357c4286959818b62e57bcbc",
            "7de734d32b95473cb44e4931875b52d1",
            "85b493493e4e4562a3db355f57ebe0b1",
            "5dc5505d11e749b196e8b5cfc42cb588",
            "de26ae4953f44fe0b6252ae0661aaafc",
            "23bf6c6f895b49c689497717cd98a1d1",
            "7be2d4ab98dc403499c139ab07fd2019",
            "6fece36129424ba19164082d28ef51ec",
            "1ae55629d068468e834ff1a9ae54d7a2",
            "9190dbc6d7804b3b9561e3deec3fef11",
            "dc3b3cbd520148e0983d41196ef891e5",
            "7d29a1ae227641e5833e09ebbcaed241"
          ]
        },
        "id": "oWahyFO20_mh",
        "outputId": "53658341-e61f-4414-c932-cf7f1215b39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-28 06:37:54--  https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/497407399/0233db07-b8db-4fc3-b026-b75d77fd7ae6?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A28%3A07Z&rscd=attachment%3B+filename%3Dembedding_model.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A27%3A33Z&ske=2025-10-28T07%3A28%3A07Z&sks=b&skv=2018-11-09&sig=%2F8H5StiPm%2F%2FEiD%2BV3xz%2FRDkja0W4qUm%2BP0gM31MMbi0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MSwibmJmIjoxNzYxNjMzNDUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.FrFoUlk6leHQU9ZRGYWIWYrxLV8F1J4a8YxEfj4AZHY&response-content-disposition=attachment%3B%20filename%3Dembedding_model.onnx&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-28 06:37:54--  https://release-assets.githubusercontent.com/github-production-release-asset/497407399/0233db07-b8db-4fc3-b026-b75d77fd7ae6?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A28%3A07Z&rscd=attachment%3B+filename%3Dembedding_model.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A27%3A33Z&ske=2025-10-28T07%3A28%3A07Z&sks=b&skv=2018-11-09&sig=%2F8H5StiPm%2F%2FEiD%2BV3xz%2FRDkja0W4qUm%2BP0gM31MMbi0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MSwibmJmIjoxNzYxNjMzNDUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.FrFoUlk6leHQU9ZRGYWIWYrxLV8F1J4a8YxEfj4AZHY&response-content-disposition=attachment%3B%20filename%3Dembedding_model.onnx&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1326578 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘./openwakeword/openwakeword/resources/models/embedding_model.onnx’\n",
            "\n",
            "./openwakeword/open 100%[===================>]   1.26M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-10-28 06:37:54 (193 MB/s) - ‘./openwakeword/openwakeword/resources/models/embedding_model.onnx’ saved [1326578/1326578]\n",
            "\n",
            "--2025-10-28 06:37:55--  https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/497407399/4bfa8f05-dd30-45f6-b47c-c55548bb5ffc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A32%3A51Z&rscd=attachment%3B+filename%3Dembedding_model.tflite&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A32%3A10Z&ske=2025-10-28T07%3A32%3A51Z&sks=b&skv=2018-11-09&sig=lZUuXVMvJVrhfLG2l1%2FwruOsYPJ%2FuiruOMMfmyjN%2Bbc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MiwibmJmIjoxNzYxNjMzNDUyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.MjADlOhme7Y-S030KZ-mnGFwxMG0fu5npfwOpW8XIA0&response-content-disposition=attachment%3B%20filename%3Dembedding_model.tflite&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-28 06:37:55--  https://release-assets.githubusercontent.com/github-production-release-asset/497407399/4bfa8f05-dd30-45f6-b47c-c55548bb5ffc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A32%3A51Z&rscd=attachment%3B+filename%3Dembedding_model.tflite&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A32%3A10Z&ske=2025-10-28T07%3A32%3A51Z&sks=b&skv=2018-11-09&sig=lZUuXVMvJVrhfLG2l1%2FwruOsYPJ%2FuiruOMMfmyjN%2Bbc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MiwibmJmIjoxNzYxNjMzNDUyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.MjADlOhme7Y-S030KZ-mnGFwxMG0fu5npfwOpW8XIA0&response-content-disposition=attachment%3B%20filename%3Dembedding_model.tflite&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1330312 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘./openwakeword/openwakeword/resources/models/embedding_model.tflite’\n",
            "\n",
            "./openwakeword/open 100%[===================>]   1.27M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-10-28 06:37:55 (180 MB/s) - ‘./openwakeword/openwakeword/resources/models/embedding_model.tflite’ saved [1330312/1330312]\n",
            "\n",
            "--2025-10-28 06:37:55--  https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/497407399/6b613c12-b693-4220-82d5-01be396893d9?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A31%3A35Z&rscd=attachment%3B+filename%3Dmelspectrogram.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A31%3A35Z&ske=2025-10-28T07%3A31%3A35Z&sks=b&skv=2018-11-09&sig=1MyE748%2B5ooswFOwh52DSnG6Pl0bdtk7N5Si9Sokzjg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MiwibmJmIjoxNzYxNjMzNDUyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.MjADlOhme7Y-S030KZ-mnGFwxMG0fu5npfwOpW8XIA0&response-content-disposition=attachment%3B%20filename%3Dmelspectrogram.onnx&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-28 06:37:55--  https://release-assets.githubusercontent.com/github-production-release-asset/497407399/6b613c12-b693-4220-82d5-01be396893d9?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A31%3A35Z&rscd=attachment%3B+filename%3Dmelspectrogram.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A31%3A35Z&ske=2025-10-28T07%3A31%3A35Z&sks=b&skv=2018-11-09&sig=1MyE748%2B5ooswFOwh52DSnG6Pl0bdtk7N5Si9Sokzjg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MiwibmJmIjoxNzYxNjMzNDUyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.MjADlOhme7Y-S030KZ-mnGFwxMG0fu5npfwOpW8XIA0&response-content-disposition=attachment%3B%20filename%3Dmelspectrogram.onnx&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1087958 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘./openwakeword/openwakeword/resources/models/melspectrogram.onnx’\n",
            "\n",
            "./openwakeword/open 100%[===================>]   1.04M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-10-28 06:37:55 (142 MB/s) - ‘./openwakeword/openwakeword/resources/models/melspectrogram.onnx’ saved [1087958/1087958]\n",
            "\n",
            "--2025-10-28 06:37:55--  https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/497407399/14a5e610-f7fa-4157-ae44-fdaabf875683?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A15%3A18Z&rscd=attachment%3B+filename%3Dmelspectrogram.tflite&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A15%3A16Z&ske=2025-10-28T07%3A15%3A18Z&sks=b&skv=2018-11-09&sig=0EgMZulsCPWgjLkIWA51r7ri2n%2FoBKUpGcPZVCmZCrU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MywibmJmIjoxNzYxNjMzNDUzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.d16cSo-gb-NjnY-Ls7nqiP6w-q9frqhT_gEfRqDCYqQ&response-content-disposition=attachment%3B%20filename%3Dmelspectrogram.tflite&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-28 06:37:55--  https://release-assets.githubusercontent.com/github-production-release-asset/497407399/14a5e610-f7fa-4157-ae44-fdaabf875683?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-28T07%3A15%3A18Z&rscd=attachment%3B+filename%3Dmelspectrogram.tflite&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-28T06%3A15%3A16Z&ske=2025-10-28T07%3A15%3A18Z&sks=b&skv=2018-11-09&sig=0EgMZulsCPWgjLkIWA51r7ri2n%2FoBKUpGcPZVCmZCrU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MTYzMzc1MywibmJmIjoxNzYxNjMzNDUzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.d16cSo-gb-NjnY-Ls7nqiP6w-q9frqhT_gEfRqDCYqQ&response-content-disposition=attachment%3B%20filename%3Dmelspectrogram.tflite&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1092516 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘./openwakeword/openwakeword/resources/models/melspectrogram.tflite’\n",
            "\n",
            "./openwakeword/open 100%[===================>]   1.04M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-10-28 06:37:55 (151 MB/s) - ‘./openwakeword/openwakeword/resources/models/melspectrogram.tflite’ saved [1092516/1092516]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import yaml\n",
        "import datasets\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "import locale\n",
        "import os\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# install openwakeword (full installation to support training)\n",
        "if not os.path.exists(\"./openwakeword\"):\n",
        "    !git clone https://github.com/dscripka/openwakeword\n",
        "    !pip install -e ./openwakeword --no-deps\n",
        "\n",
        "\n",
        "os.makedirs(\"./openwakeword/openwakeword/resources/models\", exist_ok=True)\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.onnx -O ./openwakeword/openwakeword/resources/models/embedding_model.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/embedding_model.tflite -O ./openwakeword/openwakeword/resources/models/embedding_model.tflite\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.onnx -O ./openwakeword/openwakeword/resources/models/melspectrogram.onnx\n",
        "!wget https://github.com/dscripka/openWakeWord/releases/download/v0.5.1/melspectrogram.tflite -O ./openwakeword/openwakeword/resources/models/melspectrogram.tflite\n",
        "\n",
        "\n",
        "\n",
        "resources_dir = \"./resources\"\n",
        "if not os.path.exists(resources_dir):\n",
        "    os.mkdir(resources_dir)\n",
        "## Download all data\n",
        "\n",
        "## Download MIR RIR data (takes about ~2 minutes)\n",
        "mit_rirs_dir =\"mit_rirs\"\n",
        "\n",
        "mit_rirs_dir_path = os.path.join(resources_dir, mit_rirs_dir)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(mit_rirs_dir_path):\n",
        "    os.mkdir(mit_rirs_dir_path)\n",
        "    !git lfs install\n",
        "    !git clone https://huggingface.co/datasets/davidscripka/MIT_environmental_impulse_responses\n",
        "    !mv MIT_environmental_impulse_responses {resources_dir}/MIT_environmental_impulse_responses\n",
        "    rir_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(os.path.join(resources_dir, \"MIT_environmental_impulse_responses\",\"16khz\")).glob(\"*.wav\")]}).cast_column(\"audio\", datasets.Audio())\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    for row in tqdm(rir_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1]\n",
        "        scipy.io.wavfile.write(os.path.join(mit_rirs_dir_path, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "## Download noise and background audio (takes about ~3 minutes)\n",
        "\n",
        "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
        "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
        "# For full-scale training, it's recommended to download the entire dataset from\n",
        "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
        "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
        "\n",
        "audioset_dir=os.path.join(resources_dir, \"audioset\")\n",
        "if not os.path.exists(audioset_dir):\n",
        "    os.mkdir(audioset_dir)\n",
        "\n",
        "    fname = \"bal_train09.tar\"\n",
        "    out_dir = os.path.join(audioset_dir, fname)\n",
        "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
        "    !wget -O {out_dir} {link}\n",
        "    !cd {audioset_dir}  && tar -xvf bal_train09.tar\n",
        "\n",
        "    output_dir = os.path.join(resources_dir, \"audioset_16k\")\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(audioset_dir, \"audio\").glob(\"**/*.flac\")]})\n",
        "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
        "    for row in tqdm(audioset_dataset):\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "\n",
        "# Free Music Archive dataset\n",
        "# https://github.com/mdeff/fma\n",
        "\n",
        "output_dir = os.path.join(resources_dir, \"fma\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
        "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
        "\n",
        "    # Save clips to 16-bit PCM wav files\n",
        "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
        "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
        "        row = next(fma_dataset)\n",
        "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
        "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
        "        i += 1\n",
        "        if i == n_hours*3600//30:\n",
        "            break\n",
        "\n",
        "# Download pre-computed openWakeWord features for training and validation\n",
        "\n",
        "# training set (~2,000 hours from the ACAV100M Dataset)\n",
        "# See https://huggingface.co/datasets/davidscripka/openwakeword_features for more information\n",
        "if not os.path.exists( os.path.join(resources_dir, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\")):\n",
        "    save_path = os.path.join(resources_dir, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\")\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/openwakeword_features_ACAV100M_2000_hrs_16bit.npy -O {save_path}\n",
        "\n",
        "# validation set for false positive rate estimation (~11 hours)\n",
        "if not os.path.exists(os.path.join(resources_dir, \"validation_set_features.npy\")):\n",
        "    save_path = os.path.join(resources_dir, \"validation_set_features.npy\")\n",
        "    !wget https://huggingface.co/datasets/davidscripka/openwakeword_features/resolve/main/validation_set_features.npy -O {save_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qgaKWIY6WlJ1",
        "outputId": "ef36ae89-c117-42a1-98d0-09dc445116c7"
      },
      "outputs": [],
      "source": [
        "# Load default YAML config file for training\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "# 生成当前时间字符串，例如 2025-10-24_15-32-45\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "target_word=\"hi_aldelo\"\n",
        "config = yaml.load(open(\"openwakeword/examples/custom_model.yml\", 'r').read(), yaml.Loader)\n",
        "\n",
        "# Modify values in the config and save a new version\n",
        "number_of_training_steps = 20000  # @param {type:\"slider\", min:0, max:50000, step:100}\n",
        "false_activation_penalty = 1500  # @param {type:\"slider\", min:100, max:5000, step:50}\n",
        "config[\"target_phrase\"] = [target_word]\n",
        "config[\"model_name\"] = f\"{config['target_phrase'][0].replace(' ', '_')}_{timestamp}\"\n",
        "config[\"steps\"] = number_of_training_steps\n",
        "config[\"output_dir\"] = \"./trained_models/\"\n",
        "config[\"max_negative_weight\"] = false_activation_penalty\n",
        "config[\"datasets\"]=\"./dataset\"\n",
        "config[\"background_paths\"] = [os.path.join(resources_dir, \"audioset_16k\"), os.path.join(resources_dir, \"fma\")]  # multiple background datasets are supported\n",
        "config[\"false_positive_validation_data_path\"] = os.path.join(resources_dir, \"validation_set_features.npy\")\n",
        "config[\"feature_data_files\"] = {\"ACAV100M_sample\": os.path.join(resources_dir, \"openwakeword_features_ACAV100M_2000_hrs_16bit.npy\")}\n",
        "config[\"rir_paths\"]= [mit_rirs_dir_path]\n",
        "\n",
        "config[\"n_samples\"] = 10\n",
        "config[\"n_samples_val\"] = 10\n",
        "config[\"negative_data\"] = \"negative_data\"\n",
        "config_yaml_path = \"my_model.yaml\"\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    documents = yaml.dump(config, file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/pronouncing/__init__.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignoring unknown arguments: ['--f=/run/user/1000/jupyter/runtime/kernel-v34107afa2f45d99960038d4fdcdb7bcda88322be0.json']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-10-28 06:38:02.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mCurrent number of positive training samples: 14992\u001b[0m\n",
            "\u001b[32m2025-10-28 06:38:02.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mCurrent number of positive testing samples: 1660\u001b[0m\n",
            "\u001b[32m2025-10-28 06:38:02.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m135\u001b[0m - \u001b[1mCurrent number of negative training samples: 18320\u001b[0m\n",
            "\u001b[32m2025-10-28 06:38:02.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mCurrent number of negative testing samples: 1043\u001b[0m\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 110483 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 83735 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 101567 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 77791 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 89679 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 86707 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 113455 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 107511 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 71843 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 95623 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 80763 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 92651 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 62927 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 65899 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 119399 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 98595 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 125343 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 116427 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 59955 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 56983 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 104539 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 68871 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
            "/tmp/ipykernel_3271282/152458878.py:152: WavFileWarning: Reached EOF prematurely; finished at 131287 bytes, expected 4294967303 bytes from header.\n",
            "  sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import optim, nn\n",
        "import torchinfo\n",
        "import torchmetrics\n",
        "import copy\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import uuid\n",
        "import numpy as np\n",
        "import scipy\n",
        "import collections\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "from openwakeword.data import augment_clips, mmap_batch_generator\n",
        "from openwakeword.utils import compute_features_from_generator\n",
        "\n",
        "from openwakeword.train import Model\n",
        "import shutil\n",
        "# Separate function to convert onnx models to tflite format\n",
        "def convert_onnx_to_tflite(onnx_model_path, output_path):\n",
        "    \"\"\"Converts an ONNX version of an openwakeword model to the Tensorflow tflite format.\"\"\"\n",
        "    # imports\n",
        "    import onnx\n",
        "    from onnx_tf.backend import prepare\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Convert to tflite from onnx model\n",
        "    onnx_model = onnx.load(onnx_model_path)\n",
        "    tf_rep = prepare(onnx_model, device=\"CPU\")\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "        tf_rep.export_graph(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(tmp_dir, \"tf_model\"))\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        logging.info(f\"####\\nSaving tflite mode to '{output_path}'\")\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "    return None\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--training_config\",\n",
        "    help=\"The path to the training config file (required)\",\n",
        "    type=str,\n",
        "    default=config_yaml_path,\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--generate_clips\",\n",
        "    help=\"Execute the synthetic data generation process\",\n",
        "    type=str,\n",
        "    default=\"True\",\n",
        ")\n",
        "parser.add_argument(    \n",
        "    \"--augment_clips\",\n",
        "    help=\"Execute the synthetic data augmentation process\",\n",
        "    type=str,\n",
        "    default=\"True\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--overwrite\",\n",
        "    help=\"Overwrite existing openwakeword features when the --augment_clips flag is used\",\n",
        "    type=str,\n",
        "    default=\"False\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--train_model\",\n",
        "    help=\"Execute the model training process\",\n",
        "    type=str,\n",
        "    default=\"True\",\n",
        ")\n",
        "\n",
        "# ✅ 忽略 Jupyter Notebook 传入的 --f 参数\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "if unknown:\n",
        "    print(f\"Ignoring unknown arguments: {unknown}\")\n",
        "config = yaml.load(open(args.training_config, 'r').read(), yaml.Loader)\n",
        "\n",
        "# imports Piper for synthetic sample generation\n",
        "sys.path.insert(0, os.path.abspath(config[\"piper_sample_generator_path\"]))\n",
        "\n",
        "\n",
        "# Define output locations\n",
        "config[\"output_dir\"] = os.path.abspath(config[\"output_dir\"])\n",
        "if not os.path.exists(config[\"output_dir\"]):\n",
        "    os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
        "if not os.path.exists(os.path.join(config[\"output_dir\"], config[\"model_name\"])):\n",
        "    os.makedirs(os.path.join(config[\"output_dir\"], config[\"model_name\"]), exist_ok=True)\n",
        "shutil.copytree(os.path.join(config[\"datasets\"],config[\"target_phrase\"][0],\"positive_train\"), os.path.join(config[\"output_dir\"],config[\"model_name\"],\"positive_train\"))\n",
        "shutil.copytree(os.path.join(config[\"datasets\"],config[\"target_phrase\"][0],\"positive_test\"), os.path.join(config[\"output_dir\"],config[\"model_name\"],\"positive_test\"))\n",
        "shutil.copytree(os.path.join(config[\"datasets\"],config[\"negative_data\"],\"negative_train\"), os.path.join(config[\"output_dir\"],config[\"model_name\"],\"negative_train\"))\n",
        "shutil.copytree(os.path.join(config[\"datasets\"],config[\"negative_data\"],\"negative_test\"), os.path.join(config[\"output_dir\"],config[\"model_name\"],\"negative_test\"))\n",
        "\n",
        "positive_train_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"positive_train\")\n",
        "positive_test_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"positive_test\")\n",
        "negative_train_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"negative_train\")\n",
        "negative_test_output_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"], \"negative_test\")\n",
        "feature_save_dir = os.path.join(config[\"output_dir\"], config[\"model_name\"])\n",
        "\n",
        "# Get paths for impulse response and background audio files\n",
        "rir_paths = [i.path for j in config[\"rir_paths\"] for i in os.scandir(j)]\n",
        "background_paths = []\n",
        "if len(config[\"background_paths_duplication_rate\"]) != len(config[\"background_paths\"]):\n",
        "    config[\"background_paths_duplication_rate\"] = [1]*len(config[\"background_paths\"])\n",
        "for background_path, duplication_rate in zip(config[\"background_paths\"], config[\"background_paths_duplication_rate\"]):\n",
        "    background_paths.extend([i.path for i in os.scandir(background_path)]*duplication_rate)\n",
        "\n",
        "\n",
        "# Generate positive clips for training\n",
        "logging.info(\"#\"*50 + \"\\nGenerating positive clips for training\\n\" + \"#\"*50)\n",
        "if not os.path.exists(positive_train_output_dir):\n",
        "    os.makedirs(positive_train_output_dir, exist_ok=True)\n",
        "n_current_samples = len(os.listdir(positive_train_output_dir))\n",
        "logger.info(f\"Current number of positive training samples: {n_current_samples}\")\n",
        "\n",
        "# Generate positive clips for testing\n",
        "logging.info(\"#\"*50 + \"\\nGenerating positive clips for testing\\n\" + \"#\"*50)\n",
        "if not os.path.exists(positive_test_output_dir):\n",
        "    os.makedirs(positive_test_output_dir, exist_ok=True)\n",
        "n_current_samples = len(os.listdir(positive_test_output_dir))\n",
        "logger.info(f\"Current number of positive testing samples: {n_current_samples}\")\n",
        "\n",
        "\n",
        "# Generate adversarial negative clips for training\n",
        "logging.info(\"#\"*50 + \"\\nGenerating negative clips for training\\n\" + \"#\"*50)\n",
        "if not os.path.exists(negative_train_output_dir):\n",
        "    os.makedirs(negative_train_output_dir, exist_ok=True)\n",
        "n_current_samples = len(os.listdir(negative_train_output_dir))\n",
        "logger.info(f\"Current number of negative training samples: {n_current_samples}\")\n",
        "\n",
        "\n",
        "# Generate adversarial negative clips for testing\n",
        "logging.info(\"#\"*50 + \"\\nGenerating negative clips for testing\\n\" + \"#\"*50)\n",
        "if not os.path.exists(negative_test_output_dir):\n",
        "    os.makedirs(negative_test_output_dir, exist_ok=True)\n",
        "n_current_samples = len(os.listdir(negative_test_output_dir))\n",
        "logger.info(f\"Current number of negative testing samples: {n_current_samples}\")\n",
        "\n",
        "\n",
        "# Set the total length of the training clips based on the ~median generated clip duration, rounding to the nearest 1000 samples\n",
        "# and setting to 32000 when the median + 750 ms is close to that, as it's a good default value\n",
        "n = 50  # sample size\n",
        "positive_clips = [str(i) for i in Path(positive_test_output_dir).glob(\"*.wav\")]\n",
        "duration_in_samples = []\n",
        "for i in range(n):\n",
        "    sr, dat = scipy.io.wavfile.read(positive_clips[np.random.randint(0, len(positive_clips))])\n",
        "    duration_in_samples.append(len(dat))\n",
        "\n",
        "config[\"total_length\"] = int(round(np.median(duration_in_samples)/1000)*1000) + 12000  # add 750 ms to clip duration as buffer\n",
        "if config[\"total_length\"] < 32000:\n",
        "    config[\"total_length\"] = 32000  # set a minimum of 32000 samples (2 seconds)\n",
        "elif abs(config[\"total_length\"] - 32000) <= 4000:\n",
        "    config[\"total_length\"] = 32000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:115: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = PitchShift(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = BandStopFilter(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddColoredNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = AddBackgroundNoise(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/transforms_interface.py:77: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Gain(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/torch_audiomentations/core/composition.py:42: FutureWarning: Transforms now expect an `output_type` argument that currently defaults to 'tensor', will default to 'dict' in v0.12, and will be removed in v0.13. Make sure to update your code to something like:\n",
            "  >>> augment = Compose(..., output_type='dict')\n",
            "  >>> augmented_samples = augment(samples).samples\n",
            "  warnings.warn(\n",
            "/home/ubuntu/miniconda3/envs/train_openwake/lib/python3.10/site-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
            "  warnings.warn(\n",
            "Computing features: 100%|█████████▉| 936/937 [06:21<00:00,  2.45it/s]\n",
            "Trimming empty rows: 15it [00:00, 47.87it/s]                        \n",
            "Computing features: 100%|█████████▉| 1144/1145 [08:06<00:00,  2.35it/s]\n",
            "Trimming empty rows: 18it [00:00, 45.41it/s]                        \n",
            "Computing features: 100%|██████████| 103/103 [00:41<00:00,  2.50it/s]\n",
            "Trimming empty rows: 2it [00:00, 70.17it/s]               \n",
            "Computing features: 100%|██████████| 65/65 [00:27<00:00,  2.39it/s]\n",
            "Trimming empty rows: 2it [00:00, 69.15it/s]               \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Do Data Augmentation\n",
        "\n",
        "if not os.path.exists(os.path.join(feature_save_dir, \"positive_features_train.npy\")) or args.overwrite is True:\n",
        "    positive_clips_train = [str(i) for i in Path(positive_train_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "    positive_clips_train_generator = augment_clips(positive_clips_train, total_length=config[\"total_length\"],\n",
        "                                                    batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                    background_clip_paths=background_paths,\n",
        "                                                    RIR_paths=rir_paths)\n",
        "\n",
        "    positive_clips_test = [str(i) for i in Path(positive_test_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "    positive_clips_test_generator = augment_clips(positive_clips_test, total_length=config[\"total_length\"],\n",
        "                                                    batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                    background_clip_paths=background_paths,\n",
        "                                                    RIR_paths=rir_paths)\n",
        "\n",
        "    negative_clips_train = [str(i) for i in Path(negative_train_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "    negative_clips_train_generator = augment_clips(negative_clips_train, total_length=config[\"total_length\"],\n",
        "                                                    batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                    background_clip_paths=background_paths,\n",
        "                                                    RIR_paths=rir_paths)\n",
        "\n",
        "    negative_clips_test = [str(i) for i in Path(negative_test_output_dir).glob(\"*.wav\")]*config[\"augmentation_rounds\"]\n",
        "    negative_clips_test_generator = augment_clips(negative_clips_test, total_length=config[\"total_length\"],\n",
        "                                                    batch_size=config[\"augmentation_batch_size\"],\n",
        "                                                    background_clip_paths=background_paths,\n",
        "                                                    RIR_paths=rir_paths)\n",
        "\n",
        "    # Compute features and save to disk via memmapped arrays\n",
        "    logging.info(\"#\"*50 + \"\\nComputing openwakeword features for generated samples\\n\" + \"#\"*50)\n",
        "    n_cpus = os.cpu_count()\n",
        "    if n_cpus is None:\n",
        "        n_cpus = 1\n",
        "    else:\n",
        "        n_cpus = n_cpus//2\n",
        "    compute_features_from_generator(positive_clips_train_generator, n_total=len(os.listdir(positive_train_output_dir)),\n",
        "                                    clip_duration=config[\"total_length\"],\n",
        "                                    output_file=os.path.join(feature_save_dir, \"positive_features_train.npy\"),\n",
        "                                    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                    ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "    compute_features_from_generator(negative_clips_train_generator, n_total=len(os.listdir(negative_train_output_dir)),\n",
        "                                    clip_duration=config[\"total_length\"],\n",
        "                                    output_file=os.path.join(feature_save_dir, \"negative_features_train.npy\"),\n",
        "                                    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                    ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "\n",
        "    compute_features_from_generator(positive_clips_test_generator, n_total=len(os.listdir(positive_test_output_dir)),\n",
        "                                    clip_duration=config[\"total_length\"],\n",
        "                                    output_file=os.path.join(feature_save_dir, \"positive_features_test.npy\"),\n",
        "                                    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                    ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "\n",
        "    compute_features_from_generator(negative_clips_test_generator, n_total=len(os.listdir(negative_test_output_dir)),\n",
        "                                    clip_duration=config[\"total_length\"],\n",
        "                                    output_file=os.path.join(feature_save_dir, \"negative_features_test.npy\"),\n",
        "                                    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                    ncpu=n_cpus if not torch.cuda.is_available() else 1)\n",
        "else:\n",
        "    logging.warning(\"Openwakeword features already exist, skipping data augmentation and feature generation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████▉| 19999/20000 [03:49<00:00, 87.02it/s] \n",
            "Training: 100%|█████████▉| 1999/2000.0 [01:56<00:00, 17.15it/s]\n",
            "Training: 100%|█████████▉| 1999/2000.0 [01:55<00:00, 17.29it/s]\n",
            "2025-10-28 07:01:35.445649: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:absl:Function `__call__` contains input name(s) onnx_tf__tf_Flatten_0_847913fa with unsupported characters which will be renamed to onnx_tf__tf_flatten_0_847913fa in the SavedModel.\n",
            "2025-10-28 07:01:37.759028: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated count of arithmetic ops: 0.101 M  ops, equivalently 0.050 M  MACs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-28 07:01:37.984857: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
            "2025-10-28 07:01:37.984882: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
            "2025-10-28 07:01:37.985527: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmprdqwcpru/tf_model\n",
            "2025-10-28 07:01:37.985969: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
            "2025-10-28 07:01:37.985982: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmprdqwcpru/tf_model\n",
            "2025-10-28 07:01:37.987710: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
            "2025-10-28 07:01:38.001740: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmprdqwcpru/tf_model\n",
            "2025-10-28 07:01:38.008157: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 22632 microseconds.\n",
            "2025-10-28 07:01:38.015227: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-10-28 07:01:38.031998: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 0.101 M  ops, equivalently 0.050 M  MACs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create openwakeword model\n",
        "from openwakeword.utils import AudioFeatures\n",
        "F = AudioFeatures(device='cpu')\n",
        "input_shape = np.load(os.path.join(feature_save_dir, \"positive_features_test.npy\")).shape[1:]\n",
        "\n",
        "oww = Model(n_classes=1, input_shape=input_shape, model_type=config[\"model_type\"],\n",
        "            layer_dim=config[\"layer_size\"], seconds_per_example=1280*input_shape[0]/16000)\n",
        "\n",
        "# Create data transform function for batch generation to handle differ clip lengths (todo: write tests for this)\n",
        "def f(x, n=input_shape[0]):\n",
        "    \"\"\"Simple transformation function to ensure negative data is the appropriate shape for the model size\"\"\"\n",
        "    if n > x.shape[1] or n < x.shape[1]:\n",
        "        x = np.vstack(x)\n",
        "        new_batch = np.array([x[i:i+n, :] for i in range(0, x.shape[0]-n, n)])\n",
        "    else:\n",
        "        return x\n",
        "    return new_batch\n",
        "\n",
        "# Create label transforms as needed for model (currently only supports binary classification models)\n",
        "data_transforms = {key: f for key in config[\"feature_data_files\"].keys()}\n",
        "label_transforms = {}\n",
        "for key in [\"positive\"] + list(config[\"feature_data_files\"].keys()) + [\"adversarial_negative\"]:\n",
        "    if key == \"positive\":\n",
        "        label_transforms[key] = lambda x: [1 for i in x]\n",
        "    else:\n",
        "        label_transforms[key] = lambda x: [0 for i in x]\n",
        "\n",
        "# Add generated positive and adversarial negative clips to the feature data files dictionary\n",
        "config[\"feature_data_files\"]['positive'] = os.path.join(feature_save_dir, \"positive_features_train.npy\")\n",
        "config[\"feature_data_files\"]['adversarial_negative'] = os.path.join(feature_save_dir, \"negative_features_train.npy\")\n",
        "\n",
        "# Make PyTorch data loaders for training and validation data\n",
        "batch_generator = mmap_batch_generator(\n",
        "    config[\"feature_data_files\"],\n",
        "    n_per_class=config[\"batch_n_per_class\"],\n",
        "    data_transform_funcs=data_transforms,\n",
        "    label_transform_funcs=label_transforms\n",
        ")\n",
        "\n",
        "class IterDataset(torch.utils.data.IterableDataset):\n",
        "    def __init__(self, generator):\n",
        "        self.generator = generator\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator\n",
        "\n",
        "n_cpus = os.cpu_count()\n",
        "if n_cpus is None:\n",
        "    n_cpus = 1\n",
        "else:\n",
        "    n_cpus = n_cpus//2\n",
        "X_train = torch.utils.data.DataLoader(IterDataset(batch_generator),\n",
        "                                        batch_size=None, num_workers=n_cpus, prefetch_factor=16)\n",
        "\n",
        "X_val_fp = np.load(config[\"false_positive_validation_data_path\"])\n",
        "X_val_fp = np.array([X_val_fp[i:i+input_shape[0]] for i in range(0, X_val_fp.shape[0]-input_shape[0], 1)])  # reshape to match model\n",
        "X_val_fp_labels = np.zeros(X_val_fp.shape[0]).astype(np.float32)\n",
        "X_val_fp = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.from_numpy(X_val_fp), torch.from_numpy(X_val_fp_labels)),\n",
        "    batch_size=len(X_val_fp_labels)\n",
        ")\n",
        "\n",
        "X_val_pos = np.load(os.path.join(feature_save_dir, \"positive_features_test.npy\"))\n",
        "X_val_neg = np.load(os.path.join(feature_save_dir, \"negative_features_test.npy\"))\n",
        "labels = np.hstack((np.ones(X_val_pos.shape[0]), np.zeros(X_val_neg.shape[0]))).astype(np.float32)\n",
        "\n",
        "X_val = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(np.vstack((X_val_pos, X_val_neg))),\n",
        "        torch.from_numpy(labels)\n",
        "        ),\n",
        "    batch_size=len(labels)\n",
        ")\n",
        "\n",
        "# Run auto training\n",
        "best_model = oww.auto_train(\n",
        "    X_train=X_train,\n",
        "    X_val=X_val,\n",
        "    false_positive_val_data=X_val_fp,\n",
        "    steps=config[\"steps\"],\n",
        "    max_negative_weight=config[\"max_negative_weight\"],\n",
        "    target_fp_per_hour=config[\"target_false_positives_per_hour\"],\n",
        ")\n",
        "\n",
        "# Export the trained model to onnx\n",
        "oww.export_model(model=best_model, model_name=config[\"model_name\"], output_dir=os.path.join(config[\"output_dir\"], config[\"model_name\"]))\n",
        "\n",
        "# Convert the model from onnx to tflite format\n",
        "convert_onnx_to_tflite(os.path.join(config[\"output_dir\"],config[\"model_name\"], config[\"model_name\"] + \".onnx\"),\n",
        "                        os.path.join(config[\"output_dir\"],config[\"model_name\"], config[\"model_name\"] + \".tflite\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "train_openwake",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ae55629d068468e834ff1a9ae54d7a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc538c08b65471a80089150ddb8e2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23bf6c6f895b49c689497717cd98a1d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc9e9c896154ded80b87ff24971a5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded8a90ee44248f68e317fcda0903634",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46c1805f82df4228b1e8eddd1d5ef00b",
            "value": 1
          }
        },
        "418b934cddf540bb989e2ce797fd1df0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c1805f82df4228b1e8eddd1d5ef00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52dd96d97087456cba48a87b2c473d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6f3a8bc9c084e5aa3f5157311c2b188",
            "placeholder": "​",
            "style": "IPY_MODEL_7a870a47357c4286959818b62e57bcbc",
            "value": " 46.3k/? [00:00&lt;00:00, 2.13MB/s]"
          }
        },
        "5dc5505d11e749b196e8b5cfc42cb588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ae55629d068468e834ff1a9ae54d7a2",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9190dbc6d7804b3b9561e3deec3fef11",
            "value": 25
          }
        },
        "6fece36129424ba19164082d28ef51ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a870a47357c4286959818b62e57bcbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7be2d4ab98dc403499c139ab07fd2019": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d29a1ae227641e5833e09ebbcaed241": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7de734d32b95473cb44e4931875b52d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85b493493e4e4562a3db355f57ebe0b1",
              "IPY_MODEL_5dc5505d11e749b196e8b5cfc42cb588",
              "IPY_MODEL_de26ae4953f44fe0b6252ae0661aaafc"
            ],
            "layout": "IPY_MODEL_23bf6c6f895b49c689497717cd98a1d1"
          }
        },
        "85b493493e4e4562a3db355f57ebe0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7be2d4ab98dc403499c139ab07fd2019",
            "placeholder": "​",
            "style": "IPY_MODEL_6fece36129424ba19164082d28ef51ec",
            "value": "Downloading readme: 100%"
          }
        },
        "9190dbc6d7804b3b9561e3deec3fef11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6f3a8bc9c084e5aa3f5157311c2b188": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8eaff451261467cb335d753da33357b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc538c08b65471a80089150ddb8e2b9",
            "placeholder": "​",
            "style": "IPY_MODEL_cbfcfeb2a4384a3a846b6162f75e1df7",
            "value": "Downloading builder script: "
          }
        },
        "c3bc30b2fd244afebb59ec17c2426094": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8eaff451261467cb335d753da33357b",
              "IPY_MODEL_3cc9e9c896154ded80b87ff24971a5e8",
              "IPY_MODEL_52dd96d97087456cba48a87b2c473d8a"
            ],
            "layout": "IPY_MODEL_418b934cddf540bb989e2ce797fd1df0"
          }
        },
        "cbfcfeb2a4384a3a846b6162f75e1df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc3b3cbd520148e0983d41196ef891e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de26ae4953f44fe0b6252ae0661aaafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc3b3cbd520148e0983d41196ef891e5",
            "placeholder": "​",
            "style": "IPY_MODEL_7d29a1ae227641e5833e09ebbcaed241",
            "value": " 25.0/25.0 [00:00&lt;00:00, 2.28kB/s]"
          }
        },
        "ded8a90ee44248f68e317fcda0903634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
